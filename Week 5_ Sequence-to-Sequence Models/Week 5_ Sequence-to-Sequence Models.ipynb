{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1228e8b1",
   "metadata": {},
   "source": [
    "# Week 5_ Sequence-to-Sequence Models\n",
    "\n",
    "- Introduction to sequence-to-sequence models and its architecture\n",
    "- Understanding of Encoder-Decoder Models and its variants\n",
    "- Introduction to attention mechanism and its role in sequence-to-sequence models\n",
    "- Understanding of Beam Search and its application in sequence-to-sequence models\n",
    "- Implementing machine translation models using PyTorch or TensorFlow\n",
    "- Understanding of evaluation metrics for machine translation\n",
    "- Understanding of transfer learning and fine-tuning pre-trained models for machine translation tasks\n",
    "- Introduction to unsupervised machine translation and its techniques\n",
    "- Understanding of Multilingual models and its application in NLP tasks\n",
    "- Understanding the concept of zero-shot learning and its application in machine translation tasks\n",
    "- Understanding the concept of back-translation and its application in machine translation tasks\n",
    "- Understanding the concept of ensembling in machine translation tasks\n",
    "- Understanding the concept of language model pre-training and its application in machine translation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92890111",
   "metadata": {},
   "source": [
    "##  Sequence-to-sequence models and its architecture\n",
    "\n",
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.\n",
    "\n",
    "<img src=\"images/sequence.jpg\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:669/0*iDgmgGnrzq65dPXy.jpg)\n",
    "\n",
    "\n",
    "<img src=\"images/sequence 1.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:700/1*y4D1XNJQmx-Gii1oHeHy_A.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53325aa",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Models and its variants\n",
    "\n",
    "#### The Encoder-Decoder Network\n",
    "\n",
    "This network have been applied to very wide range of applications including machine translation, text summarisation, questioning answering and dialogue. Let’s try to understand the idea underlying the encoder-decoder networks. The encoder takes the input sequence and creates a contextual representation (which is also called context) of it and the decoder takes this contextual representation as input and generates output sequence.\n",
    "\n",
    "<img src=\"images/encoder decoder core.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7)\n",
    "\n",
    "### Encoder:\n",
    "\n",
    "Encoder takes the input sequence and generated a context which is the essence of the input to the decoder.\n",
    "\n",
    "<img src=\"images/encoder.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7)\n",
    "\n",
    "The entire purpose of the encoder is to generate a contextual representation/ context for the input sequence.\n",
    "\n",
    "### Decoder:\n",
    "\n",
    "Decoder takes the context as input and generates a sequence of output. When we employ RNN as decoder, the context is the final hidden state of the RNN encoder.\n",
    "\n",
    "<img src=\"images/decoder.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7)\n",
    "\n",
    "The first decoder RNN cell takes “CONTEXT” as its prior hidden state. The decoder then generated the output until the end-of-sequence marker is generated.\n",
    "\n",
    "- complete encoder decoder model\n",
    "\n",
    "<img src=\"images/complete encoder decoder.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7)\n",
    "\n",
    "<img src=\"images/encoder decoder 2.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://towardsdatascience.com/what-is-an-encoder-decoder-model-86b3d57c5e1a)\n",
    "\n",
    "### Types of Encoders and Decoders\n",
    "\n",
    "There are two main types of encoder and decoder: \n",
    "- Linear \n",
    "- Nonlinear\n",
    "\n",
    "#### Linear encoders and decoders:\n",
    "\n",
    "These are the most common type. They work by taking an input signal and converting it into an output signal that is proportional to the input.\n",
    "\n",
    "#### Nonlinear encoders and decoders:\n",
    "\n",
    "These are less common but are more versatile. They work by taking an input signal and converting it into an output signal that is not proportional to the input.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bedd899",
   "metadata": {},
   "source": [
    "## Introduction to attention mechanism and its role in sequence-to-sequence models\n",
    "\n",
    "<img src=\"images/attention mechanism.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:1022/1*qhOlQHLdtfZORIXYuoCtaA.png)\n",
    "\n",
    "\n",
    "\n",
    "Seq2Seq model with an attention mechanism consists of an encoder, decoder, and attention layer.\n",
    "\n",
    "Attention layer consists of\n",
    "\n",
    "- Alignment layer\n",
    "- Attention weights\n",
    "- Context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd36cd",
   "metadata": {},
   "source": [
    "## Understanding of Beam Search and its application in sequence-to-sequence models\n",
    "\n",
    "\n",
    "<img src=\"images/beam1.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/beam2.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/beam3.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5)\n",
    "\n",
    "\n",
    "### APPLICATIONS \n",
    "\n",
    "A beam search is most often used to maintain tractability in large systems with insufficient memory to store the entire search tree.For example, \n",
    "- It has been used in many machine translation systems.\n",
    "- Each part is processed to select the best translation, and many different ways of translating the words appear.\n",
    "- According to their sentence structures, the top best translations are kept, and the rest are discarded. The translator then evaluates the translations according to a given criterion, choosing the translation which best keeps the goals.\n",
    "- The first use of a beam search was in the Harpy Speech Recognition System, CMU 1976."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697e0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
