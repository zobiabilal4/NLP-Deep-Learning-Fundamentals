## Week 5: Sequence-to-Sequence Models

- Introduction to sequence-to-sequence models and its architecture
- Understanding of Encoder-Decoder Models and its variants
- Introduction to attention mechanism and its role in sequence-to-sequence models
- Understanding of Beam Search and its application in sequence-to-sequence models
- Implementing machine translation models using PyTorch or TensorFlow
- Understanding of evaluation metrics for machine translation
- Understanding of transfer learning and fine-tuning pre-trained models for machine translation tasks
- Introduction to unsupervised machine translation and its techniques
- Understanding of Multilingual models and its application in NLP tasks
- Understanding the concept of zero-shot learning and its application in machine translation tasks
- Understanding the concept of back-translation and its application in machine translation tasks
- Understanding the concept of ensembling in machine translation tasks
- Understanding the concept of language model pre-training and its application in machine translation tasks