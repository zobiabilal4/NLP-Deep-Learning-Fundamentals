{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1731cc",
   "metadata": {},
   "source": [
    "# Week 2_ Neural Network Fundamentals\n",
    "\n",
    "- Introduction to neural networks and its architecture\n",
    "- Understanding of Perceptrons and Multi-layer perceptrons (MLP)\n",
    "- Understanding of activation functions and their role in neural networks\n",
    "- Introduction to backpropagation and gradient descent optimization algorithms\n",
    "- Understanding of overfitting and regularization techniques to prevent it\n",
    "- Understanding of Convolutional Neural Networks (CNN) and its application in NLP\n",
    "- Understanding of Recurrent Neural Networks (RNN) and its variants such as LSTM and GRU\n",
    "- Introduction to Transformer architecture and its application in NLP\n",
    "- Understanding of Generative models such as GAN and VAE\n",
    "- Understanding of Autoencoder and its application in NLP\n",
    "- Understanding of Reinforcement learning and its application in NLP\n",
    "- Introduction to Hyperparameter tuning and its importance in neural networks\n",
    "- Understanding of Batch normalization and Dropout for improving the performance of neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a0fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8b9e0cb",
   "metadata": {},
   "source": [
    "## Difference between Biological Neural Network and Artificial Neural Network\n",
    "\n",
    "<img src=\"images/comparison.png\" width =\"1000px\" height =\"1000px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf128e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "084a4417",
   "metadata": {},
   "source": [
    "## Neural networks and its architecture\n",
    "\n",
    "A neural network is a group of algorithms that certify the underlying relationship in a set of data similar to the human brain. The neural network helps to change the input so that the network gives the best result without redesigning the output procedure.\n",
    "\n",
    "<img src=\"images/neural network architecture.png\" width =\"450px\" height =\"450px\">\n",
    "\n",
    "Image source: [link to source](https://www.google.com/search?q=neural+network+and+its+architecture&sxsrf=AJOqlzVxKYJ02ECS38T8001zjVVOip5s4A:1678040533212&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiSiZqPtMX9AhUFLOwKHWSVB2EQ_AUoAXoECAEQAw&biw=1536&bih=792&dpr=1.25#imgrc=TJYs5ujUOkqz7M)\n",
    "\n",
    "\n",
    "<img src=\"images/Screenshot 2023-03-04 211153.png\" width =\"450px\" height =\"450px\">\n",
    "\n",
    "Image source: [link to source](https://www.xenonstack.com/blog/artificial-neural-network-applications)\n",
    "\n",
    "\n",
    "<img src=\"images/artificial nn.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba9b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c21ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518115d4",
   "metadata": {},
   "source": [
    "## Types of Neural Network\n",
    "\n",
    "\n",
    "<img src=\"images/types.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.xenonstack.com/blog/artificial-neural-network-applications)\n",
    "\n",
    "## Neural Networks for data-intensive applications\n",
    "\n",
    "<img src=\"images/nn app.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "<img src=\"images/nn app 1.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.xenonstack.com/blog/artificial-neural-network-applications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7aef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0a96ed",
   "metadata": {},
   "source": [
    "## Activation functions and their role in neural networks\n",
    "\n",
    "\n",
    "Activation functions are the most important part of a neural network.\n",
    "Very complicated tasks like object detection, language transformation, human face detection, object detection, etc are executed with the help of neural networks and activation functions. So, without it, these tasks are extremely complex to handle.\n",
    "\n",
    "It decides whether a neuron will be activated or not by calculating the weighted sum and further adding bias with it.\n",
    "The goal of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "Activation functions normalize the output in the range of -1 to 1 for any input.\n",
    "\n",
    "<img src=\"images/activation function.png\" width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)\n",
    "\n",
    "\n",
    "### Types of Actiavtion Functions\n",
    "\n",
    "The most commonly used activation functions are following:\n",
    "\n",
    "- Linear\n",
    "- Binary step\n",
    "- ReLU\n",
    "- LeakyReLU\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- Softmax\n",
    "\n",
    "### Linear Activation Function\n",
    "\n",
    "<img src=\"images/linear activation function.png\" width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)\n",
    "\n",
    "A simple straight line activation function, where our function is directly proportional to the weighted sum of input\n",
    "#### Equation: f(x) = mx\n",
    "\n",
    "### Binary Step Activation Function\n",
    "\n",
    "A very basic activation function, when we try to bound our output it comes to our mind every time. It is basically a classifier that classifies the output based on the threshold.\n",
    "\n",
    "In this function, we decide the threshold value.\n",
    "\n",
    "Output is greater than the threshold, neuron activated otherwise deactivated.\n",
    "\n",
    "#### Equation: f(x) = 1 if x > 0         0 if x<0\n",
    "\n",
    "For Binary classifiers or problems, we put the threshold value to be 0.\n",
    "\n",
    "### ReLU Activation Function\n",
    "\n",
    "<img src=\"images/RELU activation function.png\" width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)\n",
    "\n",
    "\n",
    "ReLU stands for Rectified Linear Unit, the most widely used activation function.\n",
    "\n",
    "Primarily used in hidden layers of artificial neural networks.\n",
    "\n",
    "Equation: f(x) = max(0,x)\n",
    "\n",
    "It gives an output x if x is positive and 0 otherwise.\n",
    "\n",
    "\n",
    "### Leaky ReLU Activation Function\n",
    "\n",
    "\n",
    "<img src=\"images/leaky RELU activation.png\" width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)\n",
    "\n",
    "Leaky ReLU function is an improved version of the ReLU activation function.\n",
    "It has a small slope for negative values instead of a flat slope.\n",
    "\n",
    "It solves the “Dying ReLU” problem, as all the negative input values turn into zero rapidly, which would deactivate the neurons in that region.\n",
    "In Leaky ReLU we do not convert all the negative inputs to zero, but near zero that solved the major issue of the ReLU activation function.\n",
    "\n",
    "#### Equation: f(x) = max(0.01*x, x)\n",
    "\n",
    "It returns x for positive input, but for negative value if x, it returns a very small value which is 0.01 times of x.\n",
    "Thus it gives an output for negative value as well.\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "\n",
    "Mostly used activation function because it does its task with great efficiency.\n",
    "It is a probabilistic approach to decision-making.\n",
    "\n",
    "#### Equation: f(x) = 1/(1+ e-x)\n",
    "\n",
    "Non-linear nature, as the x value lies between -2 to 2, y values are very steep, which means that a small change in x would bring a large change in the value of y.\n",
    "\n",
    "Range: 0 to 1\n",
    "\n",
    "Usually used in the output layer of binary classifiers, where the result is either 0 or 1.\n",
    "\n",
    "When we have to make a decision or to predict an output we use the sigmoid activation function because of its minimum range, which makes prediction more accurate.\n",
    "\n",
    "\n",
    "### Tangent Hyperbolic Activation Function(Tanh)\n",
    "\n",
    "<img src=\"images/tangent hyperbolic activation function.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://blog.knoldus.com/getting-familiar-with-activation-function-and-its-types/)\n",
    "\n",
    "\n",
    "#### Equation: f(x) = tanh(x) = 2/(1+e-2x) – 1   OR    tanh(x) = 2 * sigmoid(2x) – 1\n",
    "\n",
    "Range: -1 to 1\n",
    "\n",
    "Like sigmoid activation function, used in hidden layers as its values lie between -1 to 1 hence the mean for the hidden layer comes out to be 0 or very close to it, hence it helps in centering the data by bringing mean close to 0. This makes learning for the next layer easier and to predict or to differentiate between two classes but it maps the negative input into negative quantity only.\n",
    "\n",
    "\n",
    "### Softmax Activation Function\n",
    "\n",
    "The softmax function is also a type of sigmoid function, mostly used for classification problems.\n",
    "\n",
    "Used primarily at the last layer i.e., Output layer for decision making like sigmoid function works.\n",
    "\n",
    "Both sigmoid and softmax, considered for Binary Classification problems but when we try to handle multi-class classification problems. It would squeeze the outputs for each class between 0 and 1 and would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6bf79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c056df",
   "metadata": {},
   "source": [
    "## Backpropagation and Gradient descent optimization algorithms\n",
    "\n",
    "Backpropagation is a training algorithm used for training feedforward neural networks. It plays an important part in improving the predictions made by neural networks. This is because backpropagation is able to improve the output of the neural network iteratively.\n",
    "\n",
    "In a feedforward neural network, the input moves forward from the input layer to the output layer. Backpropagation helps improve the neural network’s output. It does this by propagating the error backward from the output layer to the input layer.\n",
    "\n",
    "<img src=\"images/back propagation.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/)\n",
    "\n",
    "To understand how backpropagation works, let’s first understand how a feedforward network works.\n",
    "\n",
    "<img src=\"images/feed foward nn.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/)\n",
    "\n",
    "\n",
    "Backpropagation allows us to readjust our weights to reduce output error. The error is propagated backward during backpropagation from the output to the input layer. This error is then used to calculate the gradient of the cost function with respect to each weight.\n",
    "\n",
    "<img src=\"images/back propagation 1.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/)\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The weights are adjusted using a process called gradient descent.\n",
    "\n",
    "Gradient descent is an optimization algorithm that is used to find the weights that minimize the cost function. Minimizing the cost function means getting to the minimum point of the cost function. So, gradient descent aims to find a weight corresponding to the cost function’s minimum point.\n",
    "\n",
    "To find this weight, we must navigate down the cost function until we find its minimum point.\n",
    "\n",
    "\n",
    "<img src=\"images/gradient descent.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/)\n",
    "\n",
    "<img src=\"images/bp vs gd.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2023/01/gradient-descent-vs-backpropagation-whats-the-difference/)\n",
    "\n",
    "\n",
    "<img src=\"images/gradient descent and back propagation.png\"  width =\"450px\" height =\"400px\">\n",
    "\n",
    "Image source: [link to source](https://test.basel.in/product/gradient-descent-back-propagation/)\n",
    "\n",
    "\n",
    "### Summarizing Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to find the weights corresponding to the cost function. It needs to descend the cost function until its minimum point to find these weights. It needs the gradient and the learning rate to descend the cost function. The gradient helps find the direction for reaching the minimum point of the cost function. The learning rate helps determine the speed at which to reach the minimum point. Upon reaching the minimum point, gradient descent finds weights corresponding to the minimum point.\n",
    "\n",
    "### Summarizing Backpropagation\n",
    "\n",
    "Backpropagation is the algorithm of calculating the gradients of the cost function with respect to the weights. Backpropagation is used to improve the output of neural networks. It does this by propagating the error in a backward direction and calculating the gradient of the cost function for each weight. These gradients are used in the process of gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf2b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f40b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fdd6cb1",
   "metadata": {},
   "source": [
    "##  Regularization techniques\n",
    "\n",
    "What is regularization?\n",
    "\n",
    "<img src=\"images/regularization tachniques.png\" width =\"700px\" height =\"700px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)\n",
    "\n",
    "Have you seen this image before? As we move towards the right in this image, our model tries to learn too well the details and the noise from the training data, which ultimately results in poor performance on the unseen data.\n",
    "In other words, while going towards the right, the complexity of the model increases such that the training error reduces but the testing error doesn’t. This is shown in the image below.\n",
    "Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model’s performance on the unseen data as well.\n",
    "\n",
    "### Regularization VS Non- Regularization\n",
    "\n",
    "<img src=\"images/regularized vs non regularized.png\" width =\"350px\" height =\"350px\">\n",
    "\n",
    "Image source: [link to source](https://ww2.mathworks.cn/en/discovery/regularization.html)\n",
    "\n",
    "### The bias-variance tradeoff: Overfitting and Underfitting\n",
    "\n",
    "\n",
    "<img src=\"images/nn prevent overfitting.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)\n",
    "\n",
    "\n",
    "<img src=\"images/correction using.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)\n",
    "\n",
    "### Underfitting\n",
    "The bias error is an error from wrong assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs. This is called underfitting.\n",
    "\n",
    "### Overfitting\n",
    "The variance is an error from sensitivity to small fluctuations in the training set. High variance may result in modeling the random noise in the training data. This is called overfitting.\n",
    "\n",
    "The bias-variance tradeoff is a term to describe the fact that we can reduce the variance by increasing the bias. Good regularization techniques strive to simultaneously minimize the two sources of error. Hence, achieving better generalization.\n",
    "\n",
    "\n",
    " \n",
    "### What regularization does to a machine learning model\n",
    "\n",
    "\n",
    "<img src=\"images/model regularization.png\" width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.reddit.com/r/learnmachinelearning/comments/w7yrog/what_regularization_does_to_a_machine_learning/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3802539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc22a1f0",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN) and its application in NLP\n",
    "\n",
    "Convolutional Neural Network is a type of Feed-Forward Neural Networks used in tasks like image analysis, natural language processing, and other complex image classification problems.\n",
    "A CNN has hidden layers of convolutional layers that form the base of ConvNets. \n",
    "\n",
    "Features refer to minute details in the image data like edges, borders, shapes, textures, objects, circles, etc.\n",
    "\n",
    "At a higher level, convolutional layers detect these patterns in the image data with the help of filters. The higher-level details are taken care of by the first few convolutional layers.\n",
    "\n",
    "The deeper the network goes, the more sophisticated the pattern searching becomes.\n",
    "\n",
    "For example, in later layers rather than edges and simple shapes, filters may detect specific objects like eyes or ears, and eventually a cat, a dog, and what not.\n",
    "\n",
    "<img src=\"images/CNN.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.v7labs.com/blog/neural-network-architectures-guide)\n",
    "\n",
    "## The Deconvolutional Neural Networks (DNN)\n",
    "\n",
    "Deconvolutional Neural Networks are CNNs that work in a reverse manner.\n",
    "\n",
    "When we use convolutional layers and max-pooling, the size of the image is reduced. To go to the original size, we use upsampling and transpose convolutional layers. Upsampling does not have trainable parameters—it just repeats the rows and columns of the image data by its corresponding sizes.\n",
    "\n",
    "<img src=\"images/DNN.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.v7labs.com/blog/neural-network-architectures-guide)\n",
    "\n",
    "Transpose Convolutional layer means applying convolutional operation and upsampling at the same time. It is represented as Conv2DTranspose (number of filters, filter size, stride). If we set stride=1, we do not have any upsampling and receive an output of the same input size.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2878fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9885daf9",
   "metadata": {},
   "source": [
    "##  Recurrent Neural Networks (RNN) \n",
    "\n",
    "Recurrent Neural Networks have the power to remember what it has learned in the past and apply it in future predictions.\n",
    "\n",
    "\n",
    "<img src=\"images/RNN.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.v7labs.com/blog/neural-network-architectures-guide)\n",
    "\n",
    "The input is in the form of sequential data that is fed into the RNN, which has a hidden internal state that gets updated every time it reads the following sequence of data in the input.\n",
    "\n",
    "The internal hidden state will be fed back to the model. The RNN produces some output at every timestamp.\n",
    "\n",
    "The mathematical representation is given below:\n",
    "\n",
    "<img src=\"images/rnn mathematic.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.v7labs.com/blog/neural-network-architectures-guide)\n",
    "\n",
    "\n",
    "## The Long Short Term Memory Network (LSTM)\n",
    "\n",
    "In RNN each of our predictions looked only one timestamp back, and it has a very short-term memory. It doesn't use any information from further back.\n",
    "\n",
    "To rectify this, we can take our Recurrent Neural Networks structure and expand it by adding some more pieces to it. \n",
    "\n",
    "The critical part that we add to this Recurrent Neural Networks is memory. We want it to be able to remember what happened many timestamps ago. To achieve this, we need to add extra structures called gates to the artificial neural network structure. \n",
    "\n",
    "<img src=\"images/LSTM.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.v7labs.com/blog/neural-network-architectures-guide)\n",
    "\n",
    "\n",
    "## Echo State Networks (ESN)\n",
    "\n",
    "Echo state Networks is a RNN with sparsely connected hidden layers with typically 1% connectivity.\n",
    "\n",
    "The connectivity and weight of hidden neurons are fixed and randomly assigned. The only weight that needs to be learned is that of the output layer. It can be seen as a linear model of the weighted input passed through all the hidden layers and the targeted output. The main idea is to keep the early layers fixed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d250f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39a6f07",
   "metadata": {},
   "source": [
    "## Transformer architecture and its application in NLP\n",
    "\n",
    "The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease.\n",
    "\n",
    "\n",
    "<img src=\"images/transformer.png\"  width =\"800px\" height =\"800px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/transformers-89034557de14)\n",
    "\n",
    "### Applications\n",
    "\n",
    "\n",
    "<img src=\"images/applications of transformer.png\"  width =\"800px\" height =\"800px\">\n",
    "\n",
    "Image source: [link to source](https://www.researchgate.net/publication/356159551/figure/fig1/AS:1089187616956417@1636693972806/Some-applications-of-transformers-in-different-fields-of-machine-learning-For-NLP-a.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce29f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c1b6425",
   "metadata": {},
   "source": [
    "## Generative models such as GAN and VAE\n",
    "\n",
    "\n",
    "### Generative Adversarial Network (GAN)\n",
    "\n",
    "Generative modeling comes under the umbrella of unsupervised learning, where new/synthetic data is generated based on the patterns discovered from the input set of data.\n",
    "GAN is a generative model and is used to generate entirely new synthetic data by learning the pattern and hence is an active area of AI research. \n",
    "\n",
    "<img src=\"images/generative.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/614fc742b1fa51cbd373f389_generative-adversarial-network.png)\n",
    "\n",
    "They have two components—a generator and a discriminator that work in a competitive fashion. \n",
    "\n",
    "\n",
    "<img src=\"images/vae and g.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.researchgate.net/publication/355850728/figure/fig4/AS:1085699449200644@1635862328517/Examples-of-generative-models-From-top-to-bottom-Variational-Autoencoder-VAE-and.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61787b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c270d43",
   "metadata": {},
   "source": [
    "##  Autoencoder and its application in NLP\n",
    "\n",
    "What is Auto-Encoder?\n",
    "\n",
    "Autoencoder is an unsupervised neural network that tries to reconstruct the output layer as similar as the input layer. An autoencoder architecture has two parts:\n",
    "\n",
    "- Encoder: Mapping from Input space to lower dimension space\n",
    "- Decoder: Reconstructing from lower dimension space to Output space\n",
    "\n",
    "<img src=\"images/auto encoders.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "### Applications\n",
    "\n",
    "### 1) Dimensionality Reduction:\n",
    "\n",
    "Autoencoders train the network to explain the natural structure in the data into efficient lower-dimensional representation. It does this by using decoding and encoding strategy to minimize the reconstruction error.\n",
    "\n",
    "<img src=\"images/dimension reduction.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "\n",
    "### 2) Feature Extraction:\n",
    "\n",
    "Autoencoders can be used as a feature extractor for classification or regression tasks. Autoencoders take un-labeled data and learn efficient codings about the structure of the data that can be used for supervised learning tasks.\n",
    "\n",
    "<img src=\"images/feature extraction auto encoder application.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "### 3) Image Denoising:\n",
    "\n",
    "The real-world raw input data is often noisy in nature, and to train a robust supervised model requires cleaned and noiseless data. Autoencoders can be used to denoise the data.\n",
    "\n",
    "<img src=\"images/auto encoder application image noising.png\"  width =\"700px\" height =\"700px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "### 4) Image Compression:\n",
    "\n",
    "Image compression is another application of an autoencoder network. The raw input image can be passed to the encoder network and obtained a compressed dimension of encoded data. The autoencoder network weights can be learned by reconstructing the image from the compressed encoding using a decoder network.\n",
    "\n",
    "<img src=\"images/image compression auto encoder.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "### 5) Image Search:\n",
    "\n",
    "Autoencoders can be used to compress the database of images. The compressed embedding can be compared or searched with an encoded version of the search image.\n",
    "\n",
    "<img src=\"images/image search auto encoder.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "### 6) Anomaly Detection:\n",
    "\n",
    "Anomaly detection is another useful application of an autoencoder network. An anomaly detection model can be used to detect a fraudulent transaction or any highly imbalanced supervised tasks.\n",
    "\n",
    "### 7) Missing Value Imputation:\n",
    "\n",
    "Denoising autoencoders can be used to impute the missing values in the dataset. The idea is to train an autoencoder network by randomly placing missing values in the input data and trying to reconstruct the original raw data by minimizing the reconstruction loss.\n",
    "\n",
    "<img src=\"images/missing valu imputation.png\"  width =\"900px\" height =\"900px\">\n",
    "\n",
    "Image source: [link to source](https://towardsdatascience.com/6-applications-of-auto-encoders-every-data-scientist-should-know-dc703cbc892b)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a700d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8e2edd",
   "metadata": {},
   "source": [
    "## Reinforcement learning and its application in NLP\n",
    "\n",
    "Reinforcement learning is a sub-domain of machine learning that deals with training AI models to yield the maximum reward possible from a process or task assigned to them. The most optimal path or behavior is encouraged in the AI model by giving it negative inputs every time it causes the undesired outcome from a task. AI-based reinforcement learning derives its fundamentals from human psychology research, wherein good behavior is rewarded and bad behavior patterns are punished.\n",
    "\n",
    "<img src=\"images/reinforce.jpg\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://insights.daffodilsw.com/hs-fs/hubfs/Allen/info.jpg?width=1371&name=info.jpg)\n",
    "\n",
    "In the diagram given above, we assume that the main agent committing the action is an AI model. Actions are performed based on a list of norms pre-programmed into the AI model which we can refer to as the 'policy'. When a reinforcement learning algorithm is introduced into the natural flow of an AI task, it changes certain things.\n",
    "\n",
    "Every time an action is performed, based on the outcome, the algorithm decides whether to make changes in the underlying policy. When the outcome is as desired, the policy remains unchanged, but otherwise, a policy update takes place via the reinforcement learning algorithm. After the policy is updated, the AI model performs the same action differently and this goes on until the most optimal outcome is achieved repeatedly.\n",
    "\n",
    "<img src=\"images/reinforcement learning.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "\n",
    "### Applications\n",
    "\n",
    "<img src=\"images/RL app.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://www.v7labs.com/blog/reinforcement-learning-applications)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93296aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec5cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa564f7",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and its importance in neural networks\n",
    "\n",
    "<img src=\"images/hyper parameter tunning.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://miro.medium.com/v2/resize:fit:828/format:webp/1*HpfPYwNLjrJJy2ewkdk8hw.png)\n",
    "\n",
    "\n",
    "<img src=\"images/hyperparameter.png\"  width =\"600px\" height =\"600px\">\n",
    "\n",
    "Image source: [link to source](https://editor.analyticsvidhya.com/uploads/64801HPTT.png)\n",
    "\n",
    "### Importance\n",
    "\n",
    "Hyperparameter tuning takes advantage of the processing infrastructure of Google Cloud to test different hyperparameter configurations when training your model. It can give you optimized values for hyperparameters, which maximizes your model's predictive accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ce09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80bba315",
   "metadata": {},
   "source": [
    "##  Batch normalization \n",
    "\n",
    "By normalizing the inputs we are able to bring all the inputs features to the same scale. In the neural network, we need to compute the pre-activation for the first neuron of the first layer a₁₁. We know that pre-activation is nothing but the weighted sum of inputs plus bias. In other words, it is the dot product between the first row of the weight matrix W₁ and the input matrix X plus bias b₁₁.\n",
    "\n",
    "<img src=\"images/batch normal.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://miro.medium.com/v2/resize:fit:640/format:webp/1*iJm0g1Od7ekugwTfJY-iUA.png)\n",
    "\n",
    "Why is it called batch normalization?\n",
    "\n",
    "Since we are computing the mean and standard deviation from a single batch as opposed to computing it from the entire data. Batch normalization is done individually at each hidden neuron in the network.\n",
    "\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique that “drops out” or “deactivates” few neurons in the neural network randomly in order to avoid the problem of overfitting.\n",
    "\n",
    "<img src=\"images/before vs after dropout.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://miro.medium.com/v2/resize:fit:828/format:webp/1*S-Rr9boTfKusUzETeKW6Mg.png)\n",
    "\n",
    "\n",
    "<img src=\"images/drop out.png\"  width =\"500px\" height =\"500px\">\n",
    "\n",
    "Image source: [link to source](https://theaisummer.com/regularization/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a52bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e3522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
