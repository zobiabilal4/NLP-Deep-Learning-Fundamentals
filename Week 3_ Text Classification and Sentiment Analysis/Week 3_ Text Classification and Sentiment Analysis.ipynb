{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29809915",
   "metadata": {},
   "source": [
    "# Week 3_ Text Classification and Sentiment Analysis\n",
    "\n",
    "- Understanding of text classification and its types\n",
    "- Introduction to sentiment analysis and its applications\n",
    "- Understanding of Bag of Words, TF-IDF and word embeddings representations\n",
    "- Implementing text classification models using PyTorch or TensorFlow\n",
    "- Understanding of different types of architectures used in text classification such as MLP, CNN, RNN, Transformer\n",
    "- Introduction to pre-trained models such as BERT and its fine-tuning for text classification tasks\n",
    "- Understanding of evaluation metrics for text classification and sentiment analysis\n",
    "- Introduction to transfer learning for text classification\n",
    "- Understanding of active learning and its application in text classification\n",
    "- Understanding of unsupervised techniques for text classification\n",
    "- Implementing sentiment analysis models using PyTorch or TensorFlow\n",
    "- Understanding of data preparation and data cleaning for text classification and sentiment analysis tasks\n",
    "- Understanding the role of ensemble models in text classification and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47660ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0229ef1c",
   "metadata": {},
   "source": [
    "## Text classification and its types\n",
    "\n",
    "<img src= \"images/what is text classify.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://slideplayer.com/slide/16510495/)\n",
    "\n",
    "\n",
    "### Binary Text Classification\n",
    "\n",
    "<img src= \"images/TextClassification.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationExample.png)\n",
    "\n",
    "\n",
    "### Multi-class classification\n",
    "\n",
    "\n",
    "<img src= \"images/herarical classification.jpg\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://static-02.hindawi.com/articles/cin/volume-2022/1883698/figures/1883698.fig.002.jpg)\n",
    "\n",
    "\n",
    "### Hierarchical classification\n",
    "\n",
    "\n",
    "<img src= \"images/hieratical.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-3-031-17189-5_14/MediaObjects/539695_1_En_14_Fig1_HTML.png)\n",
    "\n",
    "\n",
    "### Multi-label classification\n",
    "\n",
    "\n",
    "<img src= \"images/multiclass vs multi lable.jpeg\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://editor.analyticsvidhya.com/uploads/62036WhatsApp%20Image%202021-07-19%20at%2014.31.27.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "<img src= \"images/multiclass.jpg\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:1400/1*JAXmOAImcf683aXaBDPPVg.jpeg)\n",
    "\n",
    "\n",
    "<img src= \"images/multi class.png\" width=\"600px\" hight=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://abeyon.com/wp-content/uploads/2019/08/Multiclass-Classification.png)\n",
    "\n",
    "\n",
    "<img src= \"images/multi l vs c.jpeg\" width=\"800px\" height=\"800px\">\n",
    "\n",
    "Image source: [Link to source](https://editor.analyticsvidhya.com/uploads/79491WhatsApp%20Image%202021-07-19%20at%2014.50.57.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b88e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f08a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f5bd9d",
   "metadata": {},
   "source": [
    "## Sentiment analysis and its applications\n",
    "\n",
    "<img src = \"images/sentimental 1.png\"  width=\"400px\"  height=\"400px\">\n",
    "\n",
    "Image source: [Link to source](https://cdn-images-1.medium.com/v2/resize:fit:361/0*ga5rNPmVYBsCm-lz.)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/sentimantal analysis.png\"  width=\"500px\"  height=\"500px\">\n",
    "\n",
    "Image source: [Link to source](https://medium.com/nlplanet/what-tasks-can-i-solve-with-nlp-today-1b1823cc8cdf)\n",
    "\n",
    "\n",
    "<img src = \"images/Applications-of-sentiment-analysis.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.researchgate.net/profile/Nonita-Sharma/publication/343482552/figure/fig1/AS:934458245017600@1599803615111/Applications-of-sentiment-analysis.png)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/applications of sentimental analysis 2.png\"  width=\"500px\"  height=\"500px\">\n",
    "\n",
    "Image source: [Link to source](https://cdn-gcp.new.marutitech.com/Application_of_Sentiment_Analysis_db6548d619.png)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/sentimental 3.jpg\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.slideteam.net/media/catalog/product/cache/1280x720/e/s/essential_applications_of_customer_sentiment_analysis_slide01.jpg)\n",
    "\n",
    "\n",
    "<img src = \"images/applications of sentimental 3.jpg\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](2wCEAAkGBxISEhUQExAQFRUXGBYVFhYYGRkYGhkaFRoaFhgXHRUYHighGBsmGxsXITIhJSorLi4uFyAzODMtNygtLi0BCgoKDg0OGxAQGzAlICUvLjUvMC0uLysxLS0tLy0yLS0tKy0rKy0tKy0tLS0tLy0tLTAvKy0tLS8tLS0tLS0tLf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f5739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8e7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a740c4b4",
   "metadata": {},
   "source": [
    "##  Bag of Words, TF-IDF \n",
    "\n",
    "\n",
    "<img src = \"images/bag of words.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://media.licdn.com/dms/image/C4D12AQFKqY7ewfknsA/article-cover_image-shrink_600_2000/0/1611493279099?e=2147483647&v=beta&t=Kkc1HMZXV2j0AczQ_ZlN55WFVlneUVqVUvLEidZ1v6M)\n",
    "\n",
    "<img src = \"images/bag 2.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.researchgate.net/profile/William-Wallace-14/publication/268050206/figure/fig4/AS:669532460970014@1536640386963/Example-of-the-bag-of-word-text-representation-with-the-occurrence-of-word-as-feature.ppm)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/bag and idf.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/TF_IDF-matrix.png)\n",
    "\n",
    "\n",
    "## word embeddings representations\n",
    "\n",
    "<img src = \"images/word embeding.jpg\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://slideplayer.com/slide/13795147/85/images/6/Word+embedding+properties.jpg)\n",
    "\n",
    "\n",
    "Word embedding is the collective name for a set of language modelling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. For example,\n",
    "\n",
    "“dad” = [0.1548, 0.4848, 1.864]\n",
    "\n",
    "“mom” = [0.8785, 0.8974, 2.794]\n",
    "\n",
    "In short, word embeddings are numerical vectors representing strings.\n",
    "\n",
    "In practice, the word representations are either 100, 200 or 300-dimensional vectors and they are trained on very large texts.\n",
    "\n",
    "One very important feature of word embeddings is that similar words in a semantic sense have a smaller distance (either Euclidean, cosine or other) between them than words that have no semantic relationship. For example, words like “mom” and “dad” should be closer mathematically than the words “mom” and “ketchup” or “dad” and “butter”.\n",
    "\n",
    "The second important feature of word embeddings is that, when creating input matrices for models, no matter how many unique words we have in the text corpus, we will have the same number of columns in the input matrices. This is a huge win when compared to the one-hot encoding technique where the number of columns is usually equal to the number of unique words in a document. This number can be hundreds of thousands or even millions. Dealing with very wide input matrices is computationally very demanding.\n",
    "\n",
    "For example,\n",
    "\n",
    "Imagine a sentence: Clark likes to walk in the park.\n",
    "\n",
    "There are 7 unique words here. Using one hot encoded vector, we would represent each word by:\n",
    "\n",
    "Clark = [1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "likes = [0, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "to = [0, 0, 1, 0, 0, 0, 0]\n",
    "\n",
    "walk = [0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "in = [0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "the = [0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "park = [0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "Whereas if using 2-dimensional word embeddings we would deal with vectors of:\n",
    "\n",
    "Clark = [0.13, 0.61]\n",
    "\n",
    "likes = [0.23, 0.66]\n",
    "\n",
    "to = [0.55, 0.11]\n",
    "\n",
    "walk = [0.03, 0.01]\n",
    "\n",
    "in = [0.15, 0.69]\n",
    "\n",
    "the = [0.99, 0.00]\n",
    "\n",
    "park = [0.98, 0.12]\n",
    "\n",
    "Now imagine having n sentences. The vectors in the one-hot encoded case would grow exponentially while the embedding representation vectors of words would stay the same in size. This is why when working with a lot of texts, word embeddings are used to represent words, sentences or the whole document.\n",
    "\n",
    "<img src = \"images/feature extraction.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "\n",
    "<img src = \"images/feature extraction eg.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "<img src = \"images/figure 4.3.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "\n",
    "<img src = \"images/figure 4.4.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "\n",
    "<img src = \"images/figure 4.5.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdaafa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e35d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad268d3d",
   "metadata": {},
   "source": [
    "## Types of architectures used in text classification such as MLP, CNN, RNN, Transformer\n",
    "\n",
    "<img src = \"images/different architecture.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595961c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01eecaf3",
   "metadata": {},
   "source": [
    "##  Pre-trained models such as BERT and its fine-tuning for text classification tasks\n",
    "\n",
    "\n",
    "<img src = \"images/bert.png\"  width=\"700px\"  height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:1400/1*GVcm-gUJ5r6niWB6OsOg_w.png)\n",
    "\n",
    "\n",
    "<img src = \"images/different language models.png\"  width=\"700px\"  height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:828/format:webp/1*1LS1MQbhqhmNepO1-3tFkQ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/fine-tuning_methods.png\"  width=\"700px\"  height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://www.ruder.io/content/images/2021/02/fine-tuning_methods.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31afb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3cee23e",
   "metadata": {},
   "source": [
    "##  Evaluation metrics for Text classification and Sentiment analysis\n",
    "\n",
    "\n",
    "<img src = \"images/Text-Classification-Evaluation-Metrics.png\"  width=\"700px\"  height=\"700px\">\n",
    "\n",
    "Image source: [Link to source](https://www.researchgate.net/profile/Ashok-Basnet/publication/329910331/figure/tbl1/AS:756880842452992@1557465863142/Text-Classification-Evaluation-Metrics.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0275f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7a813c",
   "metadata": {},
   "source": [
    "## Transfer learning for Text classification\n",
    "\n",
    "\n",
    "<img src = \"images/transfer learning 2.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://miro.medium.com/v2/resize:fit:881/1*vqgRU2QEmOIfLjrCWzpjAg.png)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/transfer_learning.jpg\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/transfer_learning.jpg)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/transfer learning.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://slds-lmu.github.io/seminar_nlp_ss20/figures/02-00-transfer-learning-for-nlp/compare-classical-transferlearning-ml.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85512094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "023e9d6a",
   "metadata": {},
   "source": [
    "## Active learning and its application in text classification\n",
    "\n",
    "Labeling data can be an expensive task as it is usually performed manually by domain experts. This is cumbersome for deep learning, as it is dependent on large labeled datasets. Active learning (AL) is a paradigm that aims to reduce labeling effort by only using the data which the used model deems most informative. Little research has been done on AL in a text classification setting and next to none has involved the more recent, state-of-the-art Natural Language Processing (NLP) models.\n",
    "\n",
    "\n",
    "<img src = \"images/active learning.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.semanticscholar.org/paper/Active-learning-for-text-classification-with-Hu-Namee/071d196bf069cd700008cd0586db8aeb8f048d4b)\n",
    "\n",
    "\n",
    "\n",
    "- Active learning main architecture\n",
    "\n",
    "<img src = \"images/active learning main architecture.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebd39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01036a1e",
   "metadata": {},
   "source": [
    "## Unsupervised Techniques for Text classification\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/unsupervised learning 1.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.researchgate.net/profile/Manish-Aery/publication/323377718/figure/fig2/AS:597502420070400@1519467086164/Machine-learning-techniques-include-both-unsupervised-and-supervised-learning.png)\n",
    "\n",
    "\n",
    "<img src = \"images/Taxonomy-of-unsupervised-learning.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.researchgate.net/profile/Khalid-Raza-2/publication/321059768/figure/fig8/AS:563561921343490@1511375040999/Taxonomy-of-unsupervised-learning.png)\n",
    "\n",
    "\n",
    "<img src = \"images/unsupervised learning.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/07/Unsupervised-Learning-in-ML-1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58709a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8811380c",
   "metadata": {},
   "source": [
    "## Data preparation and Data cleaning for Text classification and Sentiment analysis tasks\n",
    "\n",
    "\n",
    "<img src = \"images/data prepration.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "\n",
    "<img src = \"images/figure 4.1.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://webthesis.biblio.polito.it/10363/1/tesi.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e6eba33",
   "metadata": {},
   "source": [
    "## Role of ensemble models in text classification and sentiment analysis\n",
    "\n",
    "What are Ensemble Methods?\n",
    "\n",
    "Ensemble methods are techniques that aim at improving the accuracy of results in models by combining multiple models instead of using a single model. The combined models increase the accuracy of the results significantly. This has boosted the popularity of ensemble methods in machine learning.\n",
    "\n",
    "<img src = \"images/essemble.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://cdn.corporatefinanceinstitute.com/assets/ensemble-methods.png)\n",
    "\n",
    "\n",
    "<img src = \"images/essemble model.jpg\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://ars.els-cdn.com/content/image/1-s2.0-S266591742200068X-gr1.jpg)\n",
    "\n",
    "<img src = \"images/Schematic- ensemble model.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.researchgate.net/profile/Adrian-Oriordan/publication/308022827/figure/fig1/AS:452989816184832@1485012596755/Schematic-of-the-approach-used-in-this-research.png)\n",
    "\n",
    "\n",
    "<img src = \"images/essemble model algo.png\"  width=\"600px\"  height=\"600px\">\n",
    "\n",
    "Image source: [Link to source](https://www.mdpi.com/sensors/sensors-20-02559/article_deploy/html/images/sensors-20-02559-g001-550.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147ac4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145abe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30997c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123b3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
