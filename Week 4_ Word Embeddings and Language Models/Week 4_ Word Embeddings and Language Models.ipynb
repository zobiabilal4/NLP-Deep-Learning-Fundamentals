{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac012681",
   "metadata": {},
   "source": [
    "# Week 4_ Word Embeddings and Language Models\n",
    "\n",
    "\n",
    "- Introduction to word embeddings and its types\n",
    "- Understanding of word2vec and GloVe embeddings\n",
    "- Implementing word embeddings using PyTorch or TensorFlow\n",
    "- Understanding of language models and its types\n",
    "- Introduction to Markov Chain and Hidden Markov Models\n",
    "- Introduction to n-gram language models\n",
    "- Understanding of Recurrent Neural Language Models\n",
    "- Introduction to Generative Pre-training Transformer (GPT) and its variants\n",
    "- Introduction to fine-tuning pre-trained language models\n",
    "- Understanding of evaluation metrics for language models\n",
    "- Understanding of text generation using language models\n",
    "- Introduction to machine translation and its techniques\n",
    "- Understanding the role of attention mechanism in language models\n",
    "- Understanding the concept of Language model pre-training and its application in NLP tasks.\n",
    "- Understanding the concept of zero-shot learning and its application in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67aa59",
   "metadata": {},
   "source": [
    "## Word embeddings and its types\n",
    "\n",
    "<img src = \"images/word embeding.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://www.researchgate.net/profile/Hamid-Bekamiri/publication/361134482/figure/fig1/AS:1164172024918017@1654571648469/Types-of-word-embedding-techniques-Selva-and-Kanniga-2021.ppm)\n",
    "\n",
    "\n",
    "<img src = \"images/word embeding 2.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://www.mathworks.com/help/examples/textanalytics/win64/VisualizeWordEmbeddingsUsingTextScatterPlotsExample_01.png)\n",
    "\n",
    "Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. They have learned representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. These are essential for solving most Natural language processing problems.\n",
    "\n",
    "<img src = \"images/word to vec.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03000751/we1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041fd75",
   "metadata": {},
   "source": [
    "## Word2vec and GloVe embeddings\n",
    "\n",
    "Word2vec is a method to efficiently create word embeddings by using a two-layer neural network. \n",
    "The input of word2vec is a text corpus and its output is a set of vectors known as feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand.\n",
    "The Word2Vec objective function causes the words that have a similar context to have similar embeddings. Thus in this vector space, these words are really close. Mathematically, the cosine of the angle (Q) between such vectors should be close to 1, i.e. angle close to 0.\n",
    "\n",
    "<img src = \"images/w2vec.png\" width=\"350px\" height=\"350px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03000917/we3.png)\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. \n",
    "\n",
    "\n",
    "<img src = \"images/cbow and skip-grams.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03000954/we4-1068x651.png)\n",
    "\n",
    "\n",
    "### Continuous Bag-of-Words model  (CBOW)\n",
    "\n",
    "\n",
    "<img src = \"images/cbow.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03001031/we5.png)\n",
    "\n",
    "\n",
    "## Skip-gram model\n",
    "\n",
    "The working of the skip-gram model is quite similar to the CBOW but there is just a difference in the architecture of its neural network and the way the weight matrix is generated  as shown in the figure below:\n",
    "\n",
    "\n",
    "<img src = \"images/skip grams.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03001127/we7.png)\n",
    "\n",
    "\n",
    "## GloVe\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is an alternate method to create word embeddings. It is based on matrix factorization techniques on the word-context matrix. A large matrix of co-occurrence information is constructed and you count each “word” (the rows), and how frequently we see this word in some “context” (the columns) in a large corpus. Usually, we scan our corpus in the following manner: for each term, we look for context terms within some area defined by a window-size before the term and a window-size after the term. Also, we give less weight for more distant words.\n",
    "\n",
    "The number of “contexts” is, of course, large, since it is essentially combinatorial in size. So then we factorize this matrix to yield a lower-dimensional matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332fea7",
   "metadata": {},
   "source": [
    "## Language models and its types\n",
    "\n",
    "<img src = \"images/language models .png\" width=\"800px\" height=\"800px\">\n",
    "\n",
    "<img src = \"images/common examples of language models.png\" width=\"800px\" height=\"800px\">\n",
    "\n",
    "Image Source:[Further Explaination](https://insights.daffodilsw.com/blog/what-are-language-models-in-nlp)\n",
    "\n",
    "<img src = \"images/language models.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image Source:[image source](https://www.researchgate.net/profile/Mrinal-Bachute/publication/358420139/figure/fig3/AS:11431281092693731@1666938327851/Types-of-language-models-based-on-Deep-Learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d16de",
   "metadata": {},
   "source": [
    "##  Markov Chain and Hidden Markov Models\n",
    "\n",
    "On the surface, Markov Chains (MCs) and Hidden Markov Models (HMMs) look very similar.\n",
    "We’ll clarify their differences in two ways: Firstly, by diving into their mathematical details. Secondly, by considering the different problems, each one is used to solve.\n",
    "Together, this will build a deep understanding of the logic behind the models and their potential applications.\n",
    "Before doing that, let’s start with their common building block: Stochastic Processes\n",
    "\n",
    "## Stochastic Processes\n",
    "\n",
    "<img src = \"images/stochastic process.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)\n",
    "\n",
    "\n",
    "Markov and Hidden Markov models are engineered to handle data which can be represented as ‘sequence’ of observations over time. Hidden Markov models are probabilistic frameworks where the observed data are modeled as a series of outputs generated by one of several (hidden) internal states.\n",
    "\n",
    "### Markov Chains/Processes\n",
    "Consider a discrete system that can be in one of N different states, {x1,x2,....,xn}. Each state has a probability of transitioning to the other N-1 states. This allows us to construct a transition matrix, an N*N matrix of transition probabilities.\n",
    "Let’s say we observe the system for m steps and get the sequence {X1, X2,...., Xm}, where Xi is the state of the system at step i.\n",
    "What is the probability that the following observation, Xm+1, is in state xj, given our observations in the m previous steps?\n",
    "Well, for a stochastic model to be a Markov model, it must only depend on the system’s current state. This is known as the Markov Property.\n",
    "That means:\n",
    " P(Xm+1 = xj | Xm, Xm-1,..., X1) = P(Xm+1 = xj| Xm)\n",
    "\n",
    "This is true regardless of whether we have an MC or Markov Process. But what is the difference between these two?\n",
    "Surprisingly, there is not much agreement about what they encompass. Let’s settle on this definition:\n",
    "A stochastic process with a discrete state space is an MC. However, if the state space is continuous, then it is a Markov Process.\n",
    "We use a transition kernel instead of a matrix for a Markov Process. Further, we can split both Markov models in discrete or continuous time.\n",
    "### Markov Assumptions\n",
    "\n",
    "Markov models are developed based on mainly two assumptions.\n",
    "\n",
    "#### Limited Horizon assumption:\n",
    "Probability of being in a state at a time t depend only on the state at the time (t-1).\n",
    "\n",
    "<img src = \"images/limited horizon assumptions.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)\n",
    "\n",
    "That means state at time t represents enough summary of the past reasonably to predict the future. This assumption is an Order-1 Markov process. An order-k Markov process assumes conditional independence of state z_t from the states that are k + 1-time steps before it.\n",
    "\n",
    "#### Stationary Process Assumption:\n",
    "\n",
    "Conditional (probability) distribution over the next state, given the current state, doesn't change over time.\n",
    "\n",
    "<img src = \"images/stationary process assumptions.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)\n",
    "\n",
    "That means states keep on changing over time but the underlying process is stationary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/markove chain.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://www.youtube.com/watch?v=i3AkTO9HLXo)\n",
    "\n",
    "\n",
    "<img src = \"images/Markove chain 1.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/random walk markove.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/infinity walke.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[Further Explaination](https://www.youtube.com/watch?v=i3AkTO9HLXo)\n",
    "\n",
    "## Hidden Markov Model (HMM)\n",
    "When we can not observe the state themselves but only the result of some probability function(observation) of the states we utilize HMM. HMM is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states.\n",
    "\n",
    "#### Hidden Markov Model as a finite state machine\n",
    "Consider the example given below in Figure which elaborates how a person feels on different climates.\n",
    "\n",
    "<img src = \"images/HMM as finite.png/\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)\n",
    "Set of states (S) = {Happy, Grumpy}\n",
    "\n",
    "Set of hidden states (Q) = {Sunny , Rainy}\n",
    "\n",
    "State series over time = z∈ S_T\n",
    "\n",
    "Observed States for four day = {z1=Happy, z2= Grumpy, z3=Grumpy, z4=Happy}\n",
    "\n",
    "The feeling that you understand from a person emoting is called the observations since you observe them.\n",
    "The weather that influences the feeling of a person is called the hidden state since you can’t observe it.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420053f",
   "metadata": {},
   "source": [
    "##  N-gram language models\n",
    "\n",
    "<img src = \"images/n grams.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image Source:[image source](data:image/png;base64,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)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdec235",
   "metadata": {},
   "source": [
    "##  Recurrent Neural Net Language Model\n",
    "\n",
    "Recurrent Neural Net Language Model (RNNLM) is a type of neural net language models which contains the RNNs in the network. Since an RNN can deal with the variable length inputs, it is suitable for modeling the sequential data such as sentences in natural language.\n",
    "\n",
    "<img src = \"images/RNN language.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://data-flair.training/blogs/recurrent-neural-networks/)\n",
    "\n",
    "\n",
    "<img src = \"images/rnn 2.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://www.researchgate.net/profile/Le-Lu-9/publication/313021062/figure/fig3/AS:688562659917826@1541177539464/An-example-of-a-recurrent-neural-network-language-model.ppm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619911a",
   "metadata": {},
   "source": [
    "## Generative Pre-training Transformer (GPT) and its variants\n",
    "\n",
    "Generative pre-trained transformers (GPT) are a family of language models by OpenAI generally trained on a large corpus of text data to generate human-like text. They are built using several blocks of the transformer architecture. They can be fine-tuned for various natural language processing tasks such as text generation, language translation, and text classification. The \"pre-training\" in its name refers to the initial training process on a large text corpus where the model learns to predict the next word in a passage, which provides a solid foundation for the model to perform well on downstream tasks with limited amounts of task-specific data.\n",
    "<img src = \"images/gptversion.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image Source:[image source](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d615da",
   "metadata": {},
   "source": [
    "## Fine-tuning pre-trained language models\n",
    "\n",
    "Fine-tuning in NLP refers to the procedure of re-training a pre-trained language model using your own custom data. As a result of the fine-tuning procedure, the weights of the original model are updated to account for the characteristics of the domain data and the task you are interested in.\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/pre 1.png\" width=\"800px\" height=\"800px\">\n",
    "\n",
    "Image Source:[image source](https://techcommunity.microsoft.com/t5/image/serverpage/image-id/393162i63FDF098FCB8A24E)\n",
    "\n",
    "<img src = \"images/pre 2.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://assets-global.website-files.com/5fbd459f3b05914cf70496d7/60cbd3ee2cff2abf2ae008b6_finetune.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a2fd0d",
   "metadata": {},
   "source": [
    "## Evaluation metrics for language models\n",
    "\n",
    "let us understand this with the help of an example\n",
    "\n",
    "\n",
    "<img src = \"images/evaluation matrices.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://www.youtube.com/watch?v=ixX5H7RV9D0)\n",
    "\n",
    "For further understanding must watch the above link.\n",
    "\n",
    "<img src = \"images/matric 2.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://thegradient.pub/content/images/2019/10/lm-1.png)\n",
    "\n",
    "<img src = \"images/matric 1.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://miro.medium.com/v2/resize:fit:797/0*JpiWBlOFqYTPa8Ta.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49385f8b",
   "metadata": {},
   "source": [
    "## Text generation using language models\n",
    "\n",
    "When working with text data tokens are words or characters and any network that can model the probability of the next token is called language model. A language model captures the statistical structure of the text. If we are training the neural network to predict the next character, it is called Character Level Model. Similarly, we can train the model to predict the next word, given a sequence of words called Word Level Models. We are implementing character level model.\n",
    "\n",
    "<img src = \"images/text generation 2.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://www.section.io/engineering-education/text-generation-nn/)\n",
    "\n",
    "<img src = \"images/text generation.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://huggingface.co/tasks/text-generation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c28034",
   "metadata": {},
   "source": [
    "##  Machine translation and its techniques\n",
    "\n",
    "Machine translation employs artificial intelligence (AI) to automatically translate content between languages without the involvement of human linguists. Google Translate, Bing Translate, Microsoft Translator and Amazon Translate are a few popular and familiar names of machine translation tools.\n",
    "\n",
    "<img src = \"images/types of machine.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e5dd5",
   "metadata": {},
   "source": [
    "## Role of attention mechanism in language models\n",
    "\n",
    "The attention mechanism is a part of a neural architecturethat enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation. The core idea behind attention is to compute a weight distribution on the input sequence, assigning higher values to more relevant elements.\n",
    "\n",
    "The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data.\n",
    "In broad terms, Attention is one component of a network’s architecture, and is in charge of managing and quantifying the interdependence:\n",
    "- Between the input and output elements (General Attention)\n",
    "- Within the input elements (Self-Attention)\n",
    "\n",
    "Let me give you an example of how Attention works in a translation task. Say we have the sentence “How was your day”, which we would like to translate to the French version - “Comment se passe ta journée”. What the Attention component of the network will do for each word in the output sentence is map the important and relevant words from the input sentence and assign higher weights to these words, enhancing the accuracy of the output prediction.\n",
    "\n",
    "<img src = \"images/attention.jpg\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://blog.floydhub.com/attention-mechanism/)\n",
    "\n",
    "- Types of Attention\n",
    "\n",
    "<img src = \"images/types of attention.jpg\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://blog.floydhub.com/attention-mechanism/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b11a8",
   "metadata": {},
   "source": [
    "##  Language model pre-training and its application in NLP tasks\n",
    "\n",
    "In simple terms, pre-training a neural network refers to first training a model on one task or dataset. Then using the parameters or model from this training to train another model on a different task or dataset. This gives the model a head-start instead of starting from scratch.\n",
    "\n",
    "Suppose we want to classify a data set of cats and dogs. We came up with a machine learning model, ml for this classification task. Once ml is done training, we save it along with all its parameters. Now suppose we have another task to accomplish: object detection. Instead of training a new model from scratch, we use ml on the object detection dataset. We refer to this as pre-training.\n",
    "\n",
    "<img src = \"images/pre tuning.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://vitalflux.com/wp-content/uploads/2021/09/NLP-pretrained-models-300x171.png?ezimgfmt=ngcb1/notWebP)\n",
    "\n",
    "###  Applications of Pre-training\n",
    "\n",
    "- Transfer Learning\n",
    "- Classification\n",
    "- Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9881885",
   "metadata": {},
   "source": [
    "##  Zero-shot learning and its application in NLP tasks\n",
    "\n",
    "Zero-Shot Learning is the ability to detect classes that the model has never seen during training. It resembles our ability as humans to generalize and identify new things without explicit supervision.\n",
    "For example, let’s say we want to do sentiment classification and news category classification. Normally, we will train/fine-tune a new model for each dataset. In contrast, with zero-shot learning, you can perform tasks such as sentiment and news classification directly without any task-specific training.\n",
    "\n",
    "<img src = \"images/zero-shot-vs-transfer.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://amitness.com/images/zero-shot-vs-transfer.png)\n",
    "\n",
    "<img src = \"images/zero 2.jpg\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://modulai.io/wp-content/uploads/2022/08/ZSL_example_updated-1024x606.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e14717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
