{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac012681",
   "metadata": {},
   "source": [
    "# Week 4_ Word Embeddings and Language Models\n",
    "\n",
    "\n",
    "- Introduction to word embeddings and its types\n",
    "- Understanding of word2vec and GloVe embeddings\n",
    "- Implementing word embeddings using PyTorch or TensorFlow\n",
    "- Understanding of language models and its types\n",
    "- Introduction to Markov Chain and Hidden Markov Models\n",
    "- Introduction to n-gram language models\n",
    "- Understanding of Recurrent Neural Language Models\n",
    "- Introduction to Generative Pre-training Transformer (GPT) and its variants\n",
    "- Introduction to fine-tuning pre-trained language models\n",
    "- Understanding of evaluation metrics for language models\n",
    "- Understanding of text generation using language models\n",
    "- Introduction to machine translation and its techniques\n",
    "- Understanding the role of attention mechanism in language models\n",
    "- Understanding the concept of Language model pre-training and its application in NLP tasks.\n",
    "- Understanding the concept of zero-shot learning and its application in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d97669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc67aa59",
   "metadata": {},
   "source": [
    "## Word embeddings and its types\n",
    "\n",
    "<img src = \"images/word embeding.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://www.researchgate.net/profile/Hamid-Bekamiri/publication/361134482/figure/fig1/AS:1164172024918017@1654571648469/Types-of-word-embedding-techniques-Selva-and-Kanniga-2021.ppm)\n",
    "\n",
    "\n",
    "<img src = \"images/word embeding 2.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://www.mathworks.com/help/examples/textanalytics/win64/VisualizeWordEmbeddingsUsingTextScatterPlotsExample_01.png)\n",
    "\n",
    "Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. They have learned representations of text in an n-dimensional space where words that have the same meaning have a similar representation. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. These are essential for solving most Natural language processing problems.\n",
    "\n",
    "<img src = \"images/word to vec.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03000751/we1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc83df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9041fd75",
   "metadata": {},
   "source": [
    "## Word2vec and GloVe embeddings\n",
    "\n",
    "Word2vec is a method to efficiently create word embeddings by using a two-layer neural network. \n",
    "The input of word2vec is a text corpus and its output is a set of vectors known as feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand.\n",
    "The Word2Vec objective function causes the words that have a similar context to have similar embeddings. Thus in this vector space, these words are really close. Mathematically, the cosine of the angle (Q) between such vectors should be close to 1, i.e. angle close to 0.\n",
    "\n",
    "<img src = \"images/w2vec.png\" width=\"350px\" height=\"350px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03000917/we3.png)\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. \n",
    "\n",
    "\n",
    "<img src = \"images/cbow and skip-grams.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03000954/we4-1068x651.png)\n",
    "\n",
    "\n",
    "### Continuous Bag-of-Words model  (CBOW)\n",
    "\n",
    "\n",
    "<img src = \"images/cbow.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03001031/we5.png)\n",
    "\n",
    "\n",
    "## Skip-gram model\n",
    "\n",
    "The working of the skip-gram model is quite similar to the CBOW but there is just a difference in the architecture of its neural network and the way the weight matrix is generated  as shown in the figure below:\n",
    "\n",
    "\n",
    "<img src = \"images/skip grams.png\" width=\"600px\" height=\"600px\">\n",
    "\n",
    "Image Source:[image source](https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/03001127/we7.png)\n",
    "\n",
    "\n",
    "## GloVe\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is an alternate method to create word embeddings. It is based on matrix factorization techniques on the word-context matrix. A large matrix of co-occurrence information is constructed and you count each “word” (the rows), and how frequently we see this word in some “context” (the columns) in a large corpus. Usually, we scan our corpus in the following manner: for each term, we look for context terms within some area defined by a window-size before the term and a window-size after the term. Also, we give less weight for more distant words.\n",
    "\n",
    "The number of “contexts” is, of course, large, since it is essentially combinatorial in size. So then we factorize this matrix to yield a lower-dimensional matrix, where each row now yields a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1a3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7332fea7",
   "metadata": {},
   "source": [
    "## Language models and its types\n",
    "\n",
    "<img src = \"images/language models .png\" width=\"800px\" height=\"800px\">\n",
    "\n",
    "<img src = \"images/common examples of language models.png\" width=\"800px\" height=\"800px\">\n",
    "\n",
    "Image Source:[Further Explaination](https://insights.daffodilsw.com/blog/what-are-language-models-in-nlp)\n",
    "\n",
    "<img src = \"images/language models.png\" width=\"700px\" height=\"700px\">\n",
    "\n",
    "Image Source:[image source](https://www.researchgate.net/profile/Mrinal-Bachute/publication/358420139/figure/fig3/AS:11431281092693731@1666938327851/Types-of-language-models-based-on-Deep-Learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7601a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "548d16de",
   "metadata": {},
   "source": [
    "##  Markov Chain and Hidden Markov Models\n",
    "\n",
    "On the surface, Markov Chains (MCs) and Hidden Markov Models (HMMs) look very similar.\n",
    "We’ll clarify their differences in two ways: Firstly, by diving into their mathematical details. Secondly, by considering the different problems, each one is used to solve.\n",
    "Together, this will build a deep understanding of the logic behind the models and their potential applications.\n",
    "Before doing that, let’s start with their common building block: Stochastic Processes\n",
    "\n",
    "## Stochastic Processes\n",
    "\n",
    "<img src = \"images/stochastic process.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75)\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/markove chain.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[image source](https://www.youtube.com/watch?v=i3AkTO9HLXo)\n",
    "\n",
    "\n",
    "<img src = \"images/Markove chain 1.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/random walk markove.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/infinity walke.png\" width=\"400px\" height=\"400px\">\n",
    "\n",
    "Image Source:[Further Explaination](https://www.youtube.com/watch?v=i3AkTO9HLXo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f36efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0c645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f8fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cf6056d",
   "metadata": {},
   "source": [
    "<img src=\"https://play-lh.googleusercontent.com/2El-X0RCUTz3jdOrERZh3NosHTpyYznQqWvQV4gnibCJq02tLztlQGjdOio4GY-oEt8\" width=\"300px\" height=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbc1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
